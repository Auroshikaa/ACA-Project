{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e39qHUFoU1lj",
        "outputId": "7a7bbda8-690d-4916-e68c-e99c7210f51c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiling FSDP\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::copy_         0.17%     275.088us         0.33%     540.959us      20.036us      53.734ms        54.09%      53.734ms       1.990ms            27  \n",
            "                                        FSDP_Train_Step        10.55%      17.446ms        42.80%      70.756ms      70.756ms       0.000us         0.00%      33.573ms      33.573ms             1  \n",
            "                              Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us      30.169ms        30.37%      30.169ms      30.169ms             1  \n",
            "                              Optimizer.step#AdamW.step         1.31%       2.171ms         3.10%       5.119ms       5.119ms       0.000us         0.00%      30.127ms      30.127ms             1  \n",
            "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      28.117ms        28.30%      28.117ms      28.117ms             1  \n",
            "                                        gloo:all_reduce         0.00%       0.000us         0.00%       0.000us       0.000us      28.117ms        28.30%      28.117ms      28.117ms             1  \n",
            "autograd::engine::evaluate_function: torch::autograd...         0.05%      87.402us        17.95%      29.665ms      29.665ms       0.000us         0.00%      25.512ms      25.512ms             1  \n",
            "           FullyShardedDataParallel._post_backward_hook        17.75%      29.341ms        17.89%      29.565ms      29.565ms       0.000us         0.00%      25.512ms      25.512ms             1  \n",
            "                                       c10d::allreduce_         0.05%      82.354us         0.08%     129.897us     129.897us       0.000us         0.00%      25.512ms      25.512ms             1  \n",
            "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us      25.512ms        25.68%      25.512ms      25.512ms             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 165.301ms\n",
            "Self CUDA time total: 99.351ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "input_ids = tokenizer(\"The future of AI is\", return_tensors=\"pt\").input_ids.to(device)\n",
        "labels = input_ids.clone()\n",
        "\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"12355\"\n",
        "dist.init_process_group(\"gloo\", rank=0, world_size=1)\n",
        "\n",
        "fsdp_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
        "fsdp_model = FSDP(fsdp_model)\n",
        "optimizer = torch.optim.AdamW(fsdp_model.parameters(), lr=5e-5)\n",
        "\n",
        "print(\"Profiling FSDP\")\n",
        "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "             record_shapes=True,\n",
        "             with_stack=True) as prof:\n",
        "    with record_function(\"FSDP_Train_Step\"):\n",
        "        outputs = fsdp_model(input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "dist.destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PCY7ACEYQ4Ls"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}